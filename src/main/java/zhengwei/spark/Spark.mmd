Mind Map generated by NB MindMap plugin   
> __version__=`1.1`,showJumps=`true`
---

# Spark2\.4\.3

## SparkContext
> collapsed=`true`

- NOTE
<pre>SparkContext是Spark中极为重要的一个组件，它伴随着整个Spark程序的生命周期。
Spark的核心是RDD，而整个程序的第一个RDD就是由SparkContext来创建的。
SparkContext是通向Spark集群的入口。</pre>

### 主要属性
> collapsed=`true`


#### SparkConf
> collapsed=`true`


##### SparkConf是SparkContext初始化的必要前提

#### LiveListenerBus
> collapsed=`true`

- NOTE
<pre>_listenerBus = new LiveListenerBus(_conf)</pre>
- TOPIC
<pre>16D7C7C517FA</pre>

##### 事件总线，异步的将产生的源事件投递给已经注册的监听器

#### AppStatusStore
> collapsed=`true`

- NOTE
<pre>// Initialize the app status store and listener before SparkEnv is created so that it gets
// all events.
_statusStore = AppStatusStore.createLiveStore(conf)
listenerBus.addToStatusQueue(_statusStore.listener.get)
//org.apache.spark.status.AppStatusStore.createLiveStore方法
def createLiveStore(conf: SparkConf): AppStatusStore = {
	//使用ElementTrackingStore数据结构，它是能够跟踪元素及其数量的键值对存储结构，因此适用于监控。
    val store = new ElementTrackingStore(new InMemoryStore(), conf)
    //产生一个AppStatusListener监听器，并注册到LiveListenerBus上用来收集监控数据
    val listener = new AppStatusListener(store, conf, true)
    new AppStatusStore(store, listener = Some(listener))
}</pre>

##### 它提供对Spark程序中各项指标的键值对监控，可以在SparkUI中看到其内容

#### SparkEnv
> collapsed=`true`

- NOTE
<pre>SparkEnv是Spark的执行环境，Driver和Executor的运行都需要其提供的个类组件形成的环境来作为基础
// Create the Spark execution environment (cache, map output tracker, etc)
//创建SparkEnv需要依赖LiveListenerBus
_env = createSparkEnv(_conf, isLocal, listenerBus)
//在创建Driver环境创建完毕之后，会调用SparkEnv伴生对象的set()方法保存它，这样就一处创建多处使用了。
SparkEnv.set(_env)
//org.apache.spark.SparkContext.createSparkEnv方法如下
// This function allows components created by SparkEnv to be mocked in unit tests:
private[spark] def createSparkEnv(
  conf: SparkConf,
  isLocal: Boolean,
  listenerBus: LiveListenerBus): SparkEnv = {
SparkEnv.createDriverEnv(conf, isLocal, listenerBus, SparkContext.numDriverCores(master, conf))
}
SparkEnv在初始化时只会先去初始化Driver的运行环境，SparkContext会借助SparkEnv来启动BlockManager和MetricManager</pre>

##### SecurityManager

##### RpcEnv
> collapsed=`true`


###### RpcEnv和ListenerBus一样，在Spark中不可获取

##### SerializerManager

##### BroadcastManager

##### MapOutputTracker

##### ShuffleManager

##### MemoryManager

##### BlockManager

##### MetricsSystem

##### OutputCommitCoordinator

#### SparkStatusTrack
> collapsed=`true`


##### Spark运行状态追踪器

##### 只提供很弱的一致性保证

#### ConsoleProgressBar
> collapsed=`true`


##### 在控制台打印Stage计算进度

##### 由spark\.ui\.showConsoleProgress参数控制，默认为false

#### SparkUI
> collapsed=`true`


##### 维护监控数据在WebUI上展示

##### 提供org\.apache\.spark\.ui\.WebUI\.WebUI参数来控制是否要开启WebUI ，默认为true

#### Configuration
> collapsed=`true`


##### 由SparkHadoopUtil初始化一些Hadoop相关配置存放到Hadoop的Configuration实例中

##### 参数以"spark\.hadoop"开头的配置

#### HeartbeatReceiver
> collapsed=`true`


##### 所有的Executor都会向HeartbeatReceiver发送心跳，以确保Executor还存活

##### 其本质也是个监听器，当收到Executor的心跳后，首先Executor的最后可见时间，将此信息交由TaskScheduler做进一步处理

##### 其被RpcEnv包装成了一个RpcEndpointRef，心跳机制时Spark集群中确认Executor状况的一种手段

#### SchedulerBackend
> collapsed=`true`

- TOPIC
<pre>16D56C43C01A</pre>

##### 负责向等待计算的Task分配计算资源，并在Executor上启动Task

##### SchedulerBackend是一个特质，根据不同的部署模式，会实例化相应的实现类
> collapsed=`true`


###### 本地模式\-\>LocalSchedulerBackend

###### 集群模式\-\>CoarseGrainedSchedulerBackend

###### StandaloneSchedulerBackend

#### TaskScheduler
> collapsed=`true`,topicLinkUID=`16D56C43C01A`

- NOTE
<pre>val (sched, ts) = SparkContext.createTaskScheduler(this, master, deployMode)
_schedulerBackend = sched
_taskScheduler = ts
/**
* 方法比较长，它包括有三种本地模式、本地集群模式、Standalone模式，以及第三方集群管理器（如YARN）提供的模式。
* Create a task scheduler based on a given master URL.
* Return a 2-tuple of the scheduler backend and the task scheduler.
*/
private def createTaskScheduler(
  sc: SparkContext,
  master: String,
  deployMode: String): (SchedulerBackend, TaskScheduler) = {
    import SparkMasterRegex._
    
    // When running locally, don't try to re-execute tasks on failure.
    val MAX_LOCAL_TASK_FAILURES = 1
    
    master match {
      case &quot;local&quot; =&gt;
        val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true)
        val backend = new LocalSchedulerBackend(sc.getConf, scheduler, 1)
        scheduler.initialize(backend)
        (backend, scheduler)
    
      case LOCAL_N_REGEX(threads) =&gt;
        def localCpuCount: Int = Runtime.getRuntime.availableProcessors()
        // local[*] estimates the number of cores on the machine; local[N] uses exactly N threads.
        val threadCount = if (threads == &quot;*&quot;) localCpuCount else threads.toInt
        if (threadCount &lt;= 0) {
          throw new SparkException(s&quot;Asked to run locally with $threadCount threads&quot;)
        }
        val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true)
        val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount)
        scheduler.initialize(backend)
        (backend, scheduler)
    
      case LOCAL_N_FAILURES_REGEX(threads, maxFailures) =&gt;
        def localCpuCount: Int = Runtime.getRuntime.availableProcessors()
        // local[*, M] means the number of cores on the computer with M failures
        // local[N, M] means exactly N threads with M failures
        val threadCount = if (threads == &quot;*&quot;) localCpuCount else threads.toInt
        val scheduler = new TaskSchedulerImpl(sc, maxFailures.toInt, isLocal = true)
        val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount)
        scheduler.initialize(backend)
        (backend, scheduler)
    
      case SPARK_REGEX(sparkUrl) =&gt;
        val scheduler = new TaskSchedulerImpl(sc)
        val masterUrls = sparkUrl.split(&quot;,&quot;).map(&quot;spark://&quot; + _)
        val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls)
        scheduler.initialize(backend)
        (backend, scheduler)
    
      case LOCAL_CLUSTER_REGEX(numSlaves, coresPerSlave, memoryPerSlave) =&gt;
        // Check to make sure memory requested &lt;= memoryPerSlave. Otherwise Spark will just hang.
        val memoryPerSlaveInt = memoryPerSlave.toInt
        if (sc.executorMemory &gt; memoryPerSlaveInt) {
          throw new SparkException(&quot;Asked to launch cluster with %d MB RAM / worker but requested %d MB/worker&quot;.format(memoryPerSlaveInt, sc.executorMemory))
        }
    
        val scheduler = new TaskSchedulerImpl(sc)
        val localCluster = new LocalSparkCluster(numSlaves.toInt, coresPerSlave.toInt, memoryPerSlaveInt, sc.conf)
        val masterUrls = localCluster.start()
        val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls)
        scheduler.initialize(backend)
        backend.shutdownCallback = (backend: StandaloneSchedulerBackend) =&gt; {
          localCluster.stop()
        }
        (backend, scheduler)
    
      case masterUrl =&gt;
        val cm = getClusterManager(masterUrl) match {
          case Some(clusterMgr) =&gt; clusterMgr
          case None =&gt; throw new SparkException(&quot;Could not parse Master URL: '&quot; + master + &quot;'&quot;)
        }
        try {
          val scheduler = cm.createTaskScheduler(sc, masterUrl)
          val backend = cm.createSchedulerBackend(sc, masterUrl, scheduler)
          cm.initialize(scheduler, backend)
          (backend, scheduler)
        } catch {
          case se: SparkException =&gt; throw se
          case NonFatal(e) =&gt;
            throw new SparkException(&quot;External scheduler cannot be instantiated&quot;, e)
        }
    }
}</pre>

##### TaskScheduler是任务调度器，它是一个特质，但只有一个实现TaskSchedulerImpl

##### 负责Task的调度算法，并且持有SchedulerBackend的实例，借由Backend来发挥作用

##### ClusterManager的资源分配属于一级分配，ClusterManager将Worker中的计算资源分配给Application，Application通过TaskScheduler进行二级分配，将计算资源分配给具体的Task

#### DAGScheduler
> collapsed=`true`

- NOTE
<pre>_dagScheduler = new DAGScheduler(this)
// start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler's constructor
_taskScheduler.start()</pre>
- TOPIC
<pre>16D56C43C01A</pre>

##### 有向无环图调度器，用来表示RDD之间的血缘关系

##### 负责生成并提交Job，以及按照DAG将RDD和算子划分成Stage\(依照宽依赖来划分Stage\)

##### 每个Stage包含一组Task，称之为TaskSet，它们被传递到TaskScheduler

##### TaskScheduler所调度的Task是由DAGScheduler创建的，DAGScheduler是TaskScheduler的前置调度

##### DAGScheduler的初始化是直接被new出来的,但在其构造方法里也会将SparkContext中TaskScheduler的引用传进去。因此要等DAGScheduler创建后，再真正启动TaskScheduler。

#### EventLoggingListener
> collapsed=`true`


##### 用于事件持久化的监听器

##### 可以通过spark\.eventLog\.enabled参数来控制是否要开启，默认false，如果开启，它会注册到LiveListenerBus中，并将一些特定事件持久化到磁盘中

#### ExecutorAllocationManager
> collapsed=`true`

- NOTE
<pre>// Optionally scale number of executors dynamically based on workload. Exposed for testing.
val dynamicAllocationEnabled = Utils.isDynamicAllocationEnabled(_conf)
_executorAllocationManager =
  if (dynamicAllocationEnabled) {
    schedulerBackend match {
      case b: ExecutorAllocationClient =&gt;
        Some(new ExecutorAllocationManager(schedulerBackend.asInstanceOf[ExecutorAllocationClient], listenerBus, _conf,env.blockManager.master))
      case _ =&gt;
        None
    }
  } else {
    None
  }
_executorAllocationManager.foreach(_.start())</pre>

##### 是Executor的动态分配管理器，Spark会根据程序的负载情况来动态的增加或减少Executor的数量

#### ContextCleaner
> collapsed=`true`


##### 上下文清理器，内部维护着对RDD、Shuffle依赖和广播变量的弱引用，如果弱引用超出了程序的作用域，就异步的将其清理掉

##### 可以通过spark\.cleaner\.referenceTracking配置来控制是否要开启，默认是true

### 辅助属性
> collapsed=`true`


#### creationSite
> collapsed=`true`


##### org\.apache\.spark\.util\.CallSite

##### 用来描述SparkContext在哪被则创建了，描述的是代码的位置

#### allowMultipleContexts
> collapsed=`true`


##### 其表示一个JVM之中是否允许允许多个SparkContext实例

##### 可以通过spark\.driver\.allowMultipleContexts来进行配置，默认是false，如果设置成true的话会有警告

#### addJars/addFiles&\_jars/\_files
> collapsed=`true`

- NOTE
<pre>// Used to store a URL for each static file/jar together with the file's local timestamp
private[spark] val addedFiles = new ConcurrentHashMap[String, Long]().asScala
private[spark] val addedJars = new ConcurrentHashMap[String, Long]().asScala

//首先获取SparkConf中的spark.jars和spark.files的内容
_jars = Utils.getUserJars(_conf)
/**
* Return the jar files pointed by the &quot;spark.jars&quot; property. Spark internally will distribute
* these jars through file server. In the YARN mode, it will return an empty list, since YARN
* has its own mechanism to distribute jars.
*/
def getUserJars(conf: SparkConf): Seq[String] = {
    val sparkJars = conf.getOption(&quot;spark.jars&quot;)
    sparkJars.map(_.split(&quot;,&quot;)).map(_.filter(_.nonEmpty)).toSeq.flatten
}
_files = _conf.getOption(&quot;spark.files&quot;).map(_.split(&quot;,&quot;)).map(_.filter(_.nonEmpty)).toSeq.flatten</pre>

##### Spark支持在提交应用的时候用户提交自定义的文件和jar包

##### addJars/addFiles其数据结构是ConcurrentHashMap，用来维护用户自定义的jar和文件的URL路径和时间戳

##### \_jars/\_files分别接收spark\.jars/spark\.files开头的参数

#### persistentRdds
> collapsed=`true`

- NOTE
<pre>persistentRdds的初始化代码
private[spark] val persistentRdds = {
    val map: ConcurrentMap[Int, RDD[_]] = new MapMaker().weakValues().makeMap[Int, RDD[_]]()
    map.asScala
}
//持久化和反持久化一个RDD的方法
private[spark] def persistRDD(rdd: RDD[_]) {
//最终是向persistentRdds中的ConcurrentHashMap增加rdd的id
persistentRdds(rdd.id) = rdd
}

/**
* Unpersist an RDD from memory and/or disk storage
* 最终是把persistentRdds中对应的rdd的id移除，并向消息总线投递一个反持久化一个RDD的消息
*/
private[spark] def unpersistRDD(rddId: Int, blocking: Boolean = true) {
env.blockManager.master.removeRdd(rddId, blocking)
persistentRdds.remove(rddId)
listenerBus.post(SparkListenerUnpersistRDD(rddId))
}</pre>

##### Spark支持RDD的持久化，可以将RDD持久化到磁盘或内存中

##### persistentRdds内部维护着一个ConcurrentHashMap用来维护持久化的RDD的ID与其弱引用的映射关系

##### RDD可以通过自带的cache\(\)/persisit\(\)/unpersisit\(\)方法持久化与反持久化一个RDD，其底层就是操作persistentRdds中的ConcurrentHashMap

#### executeEnvs&\_executorMemory&\_sparkUser
> collapsed=`true`

- NOTE
<pre>private[spark] val executorEnvs = HashMap[String, String]()
//Executor内存可以通过spark.executor.memory配置项、SPARK_EXECUTOR_MEMORY环境变量、SPARK_MEM环境变量指定，优先级依次降低，且默认大小是1GB1024MB。
_executorMemory = _conf.getOption(&quot;spark.executor.memory&quot;)
                       .orElse(Option(System.getenv(&quot;SPARK_EXECUTOR_MEMORY&quot;)))
                       .orElse(Option(System.getenv(&quot;SPARK_MEM&quot;))
                       .map(warnSparkMem))
                       .map(Utils.memoryStringToMb)
                       .getOrElse(1024)
val sparkUser = Utils.getCurrentUserName()
//executorEnvs也会存储_executorMemory和_sparkUser
executorEnvs(&quot;SPARK_EXECUTOR_MEMORY&quot;) = executorMemory + &quot;m&quot;
executorEnvs ++= _conf.getExecutorEnv
executorEnvs(&quot;SPARK_USER&quot;) = sparkUser</pre>

##### executeEnv是一个HashMap，用来存储需要传递Executor的环境变量，将传递给正在执行Task的Executor使用

##### \_executorMemory&\_sparkUser就是executeEnv中的内容，分别代表Executor的内存大小和启动Spark的用户名

##### \_executorMemory默认大小是1G

#### checkpointDir
> collapsed=`true`

- NOTE
<pre>def setCheckpointDir(directory: String) {

    // If we are running on a cluster, log a warning if the directory is local.
    // Otherwise, the driver may attempt to reconstruct the checkpointed RDD from
    // its own local file system, which is incorrect because the checkpoint files
    // are actually on the executor machines.
    if (!isLocal &amp;&amp; Utils.nonLocalPaths(directory).isEmpty) {
      logWarning(&quot;Spark is not running in local mode, therefore the checkpoint directory &quot; +
        s&quot;must not be on the local filesystem. Directory '$directory' &quot; +
        &quot;appears to be on the local filesystem.&quot;)
    }

    checkpointDir = Option(directory).map { dir =&gt;
      val path = new Path(dir, UUID.randomUUID().toString)
      val fs = path.getFileSystem(hadoopConfiguration)
      fs.mkdirs(path)
      fs.getFileStatus(path).getPath.toString
    }
  }</pre>

##### 指定Spark的检查点的文件夹所在的hdfs路径

##### 为了能够在计算过程中如果出错的话，能够从检查点快速恢复，而不必从头重新计算整个RDD的lineage

##### SparkContext提供setCheckpointDir\(\)方法来设置检查点路径

##### 一旦设立检查点之后，Spark就会切断checkpoint之前的lineage，下次直接从checkpoint开始恢复

#### localProperties
> collapsed=`true`


##### 维护一个properties数据类型的线程本地变量，它是InheritableThreadLocal类型，继承自ThreadLocal，在后者的基础上允许本地变量从父线程到子线程的继承，也就是该Properties会沿着线程栈传递下去。

#### \_applicationId&\_applicationAttemptId
> collapsed=`true`

- NOTE
<pre>_applicationId = _taskScheduler.applicationId()
_applicationAttemptId = taskScheduler.applicationAttemptId()</pre>

##### 这两个ID都是在TaskScheduler初始化并启动之后再分配，TaskScheduler启动之后，应用的代码逻辑才真正被执行，并且可以多次尝试

#### \_shutdownHookRef
> collapsed=`true`


##### Spark程序的关闭钩子，在主动退出JVM的时候进行一些资源的清理

#### nextShuffleId&nextRddId
> collapsed=`true`

- NOTE
<pre>private val nextShuffleId = new AtomicInteger(0)
private val nextRddId = new AtomicInteger(0)</pre>

##### 这两个ID都是AtomicInteger类型保证原子性。Shuffle和RDD都需要唯一ID来进行标识，并且它们是递增的

### 后置初始化
> collapsed=`true`

- NOTE
<pre>    setupAndStartListenerBus()
    postEnvironmentUpdate()
    postApplicationStart()

    // Post init
    //等待SchedulerBackend初始化完毕
    _taskScheduler.postStartHook()
    //在度量系统注册DAGScheduler、BlockManager、ExecutorAllocationManager的度量源，以收集它们的监控指标
    _env.metricsSystem.registerSource(_dagScheduler.metricsSource)
    _env.metricsSystem.registerSource(new BlockManagerSource(_env.blockManager))
    _executorAllocationManager.foreach { e =&gt;
      _env.metricsSystem.registerSource(e.executorAllocationManagerSource)
    }

    // Make sure the context is stopped if the user forgets about it. This avoids leaving
    // unfinished event logs around after the JVM exits cleanly. It doesn't help if the JVM
    // is killed, though.
    logDebug(&quot;Adding shutdown hook&quot;) // force eager creation of logger
    //添加关闭钩子
    _shutdownHookRef = ShutdownHookManager.addShutdownHook(
      ShutdownHookManager.SPARK_CONTEXT_SHUTDOWN_PRIORITY) { () =&gt;
      logInfo(&quot;Invoking stop() from shutdown hook&quot;)
      try {
        stop()
      } catch {
        case e: Throwable =&gt;
          logWarning(&quot;Ignoring Exception while stopping SparkContext from shutdown hook&quot;, e)
      }
    }
  //标记SparkContext为活动状态
  SparkContext.setActiveContext(this, allowMultipleContexts)</pre>

#### org\.apache\.spark\.SparkContext\#setupAndStartListenerBus
> collapsed=`true`

- NOTE
<pre>  private def setupAndStartListenerBus(): Unit = {
    try {
      //获取用户自定义监听器并遍历
      conf.get(EXTRA_LISTENERS).foreach { classNames =&gt;
        //通过反射监听器
        val listeners = Utils.loadExtensions(classOf[SparkListenerInterface], classNames, conf)
        listeners.foreach { listener =&gt;
          //注册到LiveListenerBus中
          listenerBus.addToSharedQueue(listener)
          logInfo(s&quot;Registered listener ${listener.getClass().getName()}&quot;)
        }
      }
    } catch {
      case e: Exception =&gt;
        try {
          stop()
        } finally {
          throw new SparkException(s&quot;Exception when registering SparkListener&quot;, e)
        }
    }

    listenerBus.start(this, _env.metricsSystem)
    _listenerBusStarted = true
  }</pre>

##### 这个方法用于注册用户注册自定义监听器，并最终启动LiveListenerBus

##### 自定义监听器都需要实现SparkListener特质，通过spark\.extralListeners来配置

##### 然后通过Utils\.loadExtensions\(\)方法来通过反射来构建监听器的实例，并将它注册到LiveListenerBus中

#### org\.apache\.spark\.SparkContext\#postEnvironmentUpdate
> collapsed=`true`

- NOTE
<pre>  private def postEnvironmentUpdate() {
    if (taskScheduler != null) {
      //获取调度方式
      val schedulingMode = getSchedulingMode.toString
      //获得jar列表
      val addedJarPaths = addedJars.keys.toSeq
      //获得文件列表
      val addedFilePaths = addedFiles.keys.toSeq
      //一些环境细节
      val environmentDetails = SparkEnv.environmentDetails(conf, schedulingMode, addedJarPaths, addedFilePaths)
      //封装成SparkListenerEnvironmentUpdate事件
      val environmentUpdate = SparkListenerEnvironmentUpdate(environmentDetails)
      //推送到事件总线LiveListenerBus中
      listenerBus.post(environmentUpdate)
    }
  }</pre>

##### 该方法在添加自定义文件和jar的时候也会调用，因为添加新的资源会对程序的执行环境产生影响

##### 它会取得当前的自定义文件和JAR包列表，以及Spark配置、调度方式，然后通过SparkEnv\.environmentDetails\(\)方法再取得JVM参数、Java系统属性等，一同封装成SparkListenerEnvironmentUpdate事件，并投递给事件总线。

#### org\.apache\.spark\.SparkContext\#postApplicationStart
> collapsed=`true`

- NOTE
<pre>  /** Post the application start event */
  private def postApplicationStart() {
    // Note: this code assumes that the task scheduler has been initialized and has contacted
    // the cluster manager to get an application ID (in case the cluster manager provides one).
    listenerBus.post(SparkListenerApplicationStart(appName, Some(applicationId),
      startTime, sparkUser, applicationAttemptId, schedulerBackend.getDriverLogUrls))
  }</pre>

##### 向事件总线投递SparkListenerApplicationStart事件，表示Application已经启动。

### 其他功能
> collapsed=`true`


#### 生成RDD
> collapsed=`true`


##### 对内存中的数据执行并行化\(parallelize\)操作
- NOTE
<pre>  def parallelize[T: ClassTag](
      seq: Seq[T],
      numSlices: Int = defaultParallelism): RDD[T] = withScope {
    assertNotStopped()
    new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())
  }</pre>

##### 从外部存储系统中读取数据之后生成RDD
- NOTE
<pre>  def textFile(
      path: String,
      minPartitions: Int = defaultMinPartitions): RDD[String] = withScope {
    assertNotStopped()
    //使用TextInputFormat来接收读到的文件中的数据，使用LongWritable来接收文件内容的偏移量
    hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text], minPartitions).map(pair =&gt; pair._2.toString).setName(path)
  }
  def hadoopFile[K, V](
      path: String,
      inputFormatClass: Class[_ &lt;: InputFormat[K, V]],
      keyClass: Class[K],
      valueClass: Class[V],
      minPartitions: Int = defaultMinPartitions): RDD[(K, V)] = withScope {
    assertNotStopped()
    // This is a hack to enforce loading hdfs-site.xml.
    // See SPARK-11227 for details.
    FileSystem.getLocal(hadoopConfiguration)
    // A Hadoop configuration can be about 10 KB, which is pretty big, so broadcast it.
    val confBroadcast = broadcast(new SerializableConfiguration(hadoopConfiguration))
    val setInputPathsFunc = (jobConf: JobConf) =&gt; FileInputFormat.setInputPaths(jobConf, path)
    new HadoopRDD(
      this,
      confBroadcast,
      Some(setInputPathsFunc),
      inputFormatClass,
      keyClass,
      valueClass,
      minPartitions).setName(path)
  }</pre>

##### 在生成RDD的时候可以指定RDD中有多少个分区

#### 广播变量
> collapsed=`true`

- NOTE
<pre>  def broadcast[T: ClassTag](value: T): Broadcast[T] = {
    assertNotStopped()
    require(!classOf[RDD[_]].isAssignableFrom(classTag[T].runtimeClass),
      &quot;Can not directly broadcast RDDs; instead, call collect() and broadcast the result.&quot;)
    val bc = env.broadcastManager.newBroadcast[T](value, isLocal)
    val callSite = getCallSite
    logInfo(&quot;Created broadcast &quot; + bc.id + &quot; from &quot; + callSite.shortForm)
    cleaner.foreach(_.registerBroadcastForCleanup(bc))
    bc
  }</pre>

##### 广播变量是Spark中两种共享变量的一种

##### 所谓广播就是Driver向每个Executor发送一份数据的只读副本，而不需要Task来计算获取

##### 广播变量适合处理多节点跨Stage的共享数据，特别是输入量较大的数据，可以提高效率

#### 累加器
> collapsed=`true`


##### 累加器是Spark中两种共享变量的一种

##### 累加器就是一个能够累加结果的变量，最常见的用途是计数

##### 累加器在Driver创建和读取，在Executor端更新即做累加操作

##### 所有累计器都继承自AccumulatorV2抽象类，我们也可以自定义累加器

#### runJob
> collapsed=`true`

- NOTE
<pre>  def runJob[T, U: ClassTag](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) =&gt; U,
      partitions: Seq[Int],
      resultHandler: (Int, U) =&gt; Unit): Unit = {
    if (stopped.get()) {
      throw new IllegalStateException(&quot;SparkContext has been shutdown&quot;)
    }
    val callSite = getCallSite
    val cleanedFunc = clean(func)
    logInfo(&quot;Starting job: &quot; + callSite.shortForm)
    if (conf.getBoolean(&quot;spark.logLineage&quot;, false)) {
      logInfo(&quot;RDD's recursive dependencies:\n&quot; + rdd.toDebugString)
    }
    dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)
    progressBar.foreach(_.finishAll())
    rdd.doCheckpoint()
  }</pre>

##### Spark提供很多种runJob的重载来允许一个Job，即触发Action算子的执行

##### runJob其实调用的DAGScheduler中的runJob方法来允许Job，它接收需要计算的RDD的分区列表

##### 在计算完成后，将结果传回给resultHandler回调方法。在运行Job的同时，还会对RDD本身保存检查点

### SparkContext的伴生对象
> collapsed=`true`

- NOTE
<pre>伴生对象主要用来跟踪并维护SparkContext的创建与激活</pre>

#### org\.apache\.spark\.SparkContext\#createTaskScheduler
> collapsed=`true`


##### 创建TaskScheduler

##### 记录SparkContext的创建状态

### 其他的一些重要的方法
> collapsed=`true`


#### setActiveContext\(\)
> collapsed=`true`


##### 标记SparkContext为活动状态

#### createOrCreate\(\)
> collapsed=`true`


##### 改方法是创建SparkContext的好方法，没有就创建，有就使用

## RDD
> collapsed=`true`,leftSide=`true`


### A list of partitions

### A function for computing each split
> leftSide=`true`


### A list of dependencies on other RDDs
> leftSide=`true`


### Optionally, a Partitioner for key\-value RDDs
> leftSide=`true`


### Optionally, a list of preferred locations to compute each split on
> leftSide=`true`


## SparkConf
> collapsed=`true`,leftSide=`true`


### 可以直接从Java的系统属性中获取数据

### SparkConf中的key和value都不能为null

### SparkConf提供set和get方法来设置和获取属性值
> leftSide=`true`


### SparkConf会检查配置项时候过期了或配置项不符合要求，从而发出警告
> leftSide=`true`


## LinstenerBus
> collapsed=`true`,topicLinkUID=`16D7C7C517FA`

- NOTE
<pre>SparkContext初始化时，第一个初始化的组件就是ListenerBus
后面的很多组件都依赖它
监听器注册到ListenerBus上，事件被投递到ListenerBus，ListenerBus找到对应的监听器，
然后监听器去处理事件</pre>

### LinstenerBus底层维护一个CopyOnWriteArrayList集合

### 重要方法
> collapsed=`true`


#### addListener\(\)
> collapsed=`true`


##### 向ListenerBus注册监听器，直接操作CopyOnWriteArrayList，所以线程安全

#### removeListener\(\)
> collapsed=`true`


##### 将监听器从ListenerBus中移除，直接操作CopyOnWriteArrayList，所以线程安全

#### doPostEvent\(\)
> collapsed=`true`


##### 将事件event投递给监听器listener进行处理，具体实现由子类实现

#### postToAll\(\)
> collapsed=`true`


##### 这个方法将调用doPostEvent\(\)方法，将事件投递给已注册的监听器

##### 此方法是线程不安全的，在调用时需要确保只有一个线程进行调用

#### post\(\)
> collapsed=`true`


##### 会检查queuedEvents中有无缓存事件和事件总线是否启动

##### 最终调用postToQueue\(\)方法将一个事件投递出去

#### postToQueues\(\)
> collapsed=`true`


##### 将事件发送给所有队列，由AsyncEventQueue来完成投递到监听器的工作

##### 最终调用AsyncEventQueue的post方法，将事件投递出去
- TOPIC
<pre>16D7CDE0342A</pre>

### SparkListenerBus特质
> collapsed=`true`


#### 其是Spark Core内部事件总线的基类

#### 其中的监听器的类型时SparkListenerInterface的子类，所监听的事件类型是SparkListenerEvent的子类
> collapsed=`true`


##### SparkListenerInterface特质
> collapsed=`true`,topicLinkUID=`16D7CC60815A`


###### 分别定义了处理每一个事件的处理方法

###### 统一以"on\+事件名称"命名

##### SparkListenerEvent特质
> collapsed=`true`


###### 此类虽然是特质，但其中没有抽象方法

###### 类似于Java中的标记接口\(interface mark\)

###### 唯一的用处就是用于标记具体的事件

###### 事件类统一命名为“SparkListener\+事件名称”

#### 继承自ListenerBus，实现了doPostEvent\(\)方法，对事件进行匹配，并调用监听器的处理方法，如果没有匹配的方法则调用onOtherEvent

#### Spark默认事件的投递方式是同步方式，但是如果注册的监听器和事件非常多的话，将会造成事件的处理的积压和延迟

#### AsyncEventQueue
> collapsed=`true`,topicLinkUID=`16D7CBDCF66A`


##### SparkListenerBus的实现类

##### 异步队列

##### 是SparkContext中的事件总线LiveListenerBus的基础

##### 内部维护一个LinkedBlockingQueue\<SparkListenerEvent\>

##### eventQueue
> collapsed=`true`


###### 用于存储SparkListenerEvent事件的阻塞队列LinkedBlockingQueue

###### 它的大小是通过spark\.scheduler\.listenerbus\.eventqueue\.capacity来设定的，默认值为1000，如果不设置阻塞队列的大小的话，那么阻塞队列的默认大小为Integer\.MAX\_VALUE，那么会有OOM的风险

##### droppedEventsCounter & lastReportTimestamp
> collapsed=`true`


###### 用于被丢弃的事件的计数。当阻塞队列满了的时候，新产生的队列无法入队，就会被丢弃掉，日志中定期输出该计数器的值，用lastReportTimestamp记录下每次输出的时间戳，每当被输出到日志的时候，这个值将会被重置为0

##### started & stopped
> collapsed=`true`


###### 用于标记阻塞队列得状态

##### dispatchThread
> collapsed=`true`


###### 是将队列中的事件分发到各监听器的守护线程，实际上调用了dispatch\(\)分发
- TOPIC
<pre>16D7CB8A8F9A</pre>

###### 该线程中调用Utils\.tryOrStopSparkContext\(\)方法的作用在于执行代码块时如果抛出异常，就另外起一个线程关闭SparkContext。  

##### 重要方法
> collapsed=`true`


###### dispatch\(\)方法，分发事件
> collapsed=`true`,topicLinkUID=`16D7CB8A8F9A`


####### 中调用了父类的postToAll\(\)方法，将事件投递给已注册的监听器

####### 会从LinkedBlockingQueue中take一个事件，然后调用postToAll\(\)方法，将其投递出去

####### POISON\_PILL\(毒药丸\)
> collapsed=`true`


######## 是AsyncEventQueue的伴生对象中定义的一个空的SparkListenerEvent

######## 在队列停止时\(即stop方法被调用的时候\)会被放进队列中，当dispatchThread取得这个空对象的时候就会"中毒"退出 

###### post\(\)方法，向LinkedBlockingQueue中offer一个事件
> collapsed=`true`,topicLinkUID=`16D7CDE0342A`


####### 添加事件成功返回true，否则返回false

####### 内部会维护一些计数器，以便日志打印时输出，输出后清零

### LiveListenerBus\(异步事件总线\)
> collapsed=`true`


#### AsyncEventQueue是其核心
- TOPIC
<pre>16D7CBDCF66A</pre>

#### 其中的大部分属性和AsyncEventQueue中大同小异
> collapsed=`true`


##### queues
> collapsed=`true`


###### private val queues = new CopyOnWriteArrayList\[AsyncEventQueue\]\(\)

###### 维护一个AsyncEventQueue的列表，也就是说LiveListenerBus中可以有多个事件队列，采用CopyOnWriteArrayList来保证线程安全

##### queuedEvent
> collapsed=`true`


###### 维护一个SparkListenerEvent列表

###### 用途是在LiveListenerBus启动成功之前，可能已经有事件被投递，在启动之后，这些事件将首先被投递出去

#### 重要方法
> collapsed=`true`


##### addToQueue\(\)
> collapsed=`true`

- NOTE
<pre>private[spark] def addToQueue(
    listener: SparkListenerInterface,
    //监听器队列的名字
    queue: String): Unit = synchronized {
  if (stopped.get()) {
    throw new IllegalStateException(&quot;LiveListenerBus is stopped.&quot;)
  }
  queues.asScala.find(_.name == queue) match {
    //queue-&gt;AsyncEventQueue
    case Some(queue: AsyncEventQueue) =&gt;
      //往AsyncEventQueue中注册监听器
      queue.addListener(listener)
    //没有匹配到相同名字的AsyncEventQueue
    case None = &gt;
      //新建一个新的AsyncEventQueue
      val newQueue = new AsyncEventQueue(queue, conf, metrics, this)
      //往新建的AsyncEventQueue中注册监听器
      newQueue.addListener(listener)
      if (started.get()) {
        //启动新建的队列
        newQueue.start(sparkContext)
      }
      //把新建的队列加入到queues中
      queues.add(newQueue)
  }
}</pre>

###### 该方法将监听器注册到队列中去

###### 该方法接受监听器\(SparkListenerInterface\)和监听器的名字
- TOPIC
<pre>16D7CC60815A</pre>

###### 它会去成员变量queues中去寻找是否存在同名的队列\(AsyncEventQueue\)，如果存在，则调用AsyncEventQueue的父类SparkListenerBus的父类ListenerBus的addListener\(\)方法注册监听器

###### 如果没有相同的队列名的话，则会新建一个AsynvEventQueue，然后把监听器注册到队列中

## SparkEnv
> leftSide=`true`


### SparkContext在初始化事件总线\(ListenerBus\)之后，就会去初始化SaprkEnv

### SparkEnv初始化之后，与Spark相关的计算、存储和度量系统才会真正的准备好，SparkEnv中也包含很多的组件。
> leftSide=`true`


### Driver环境的创建
> leftSide=`true`


#### Dirver环境的创建是通过org\.apache\.spark\.SparkEnv\.createDriverEnv
- NOTE
<pre>private[spark] def createDriverEnv(
    conf: SparkConf,
    isLocal: Boolean,
    //数据总线
    listenerBus: LiveListenerBus,
    //申请的核心数
    numCores: Int,
    mockOutputCommitCoordinator: Option[OutputCommitCoordinator] = None): SparkEnv = {
  assert(conf.contains(DRIVER_HOST_ADDRESS),
    s&quot;${DRIVER_HOST_ADDRESS.key} is not set on the driver!&quot;)
  assert(conf.contains(&quot;spark.driver.port&quot;), &quot;spark.driver.port is not set on the driver!&quot;)
  val bindAddress = conf.get(DRIVER_BIND_ADDRESS)
  val advertiseAddress = conf.get(DRIVER_HOST_ADDRESS)
  val port = conf.get(&quot;spark.driver.port&quot;).toInt
  val ioEncryptionKey = if (conf.get(IO_ENCRYPTION_ENABLED)) {
    Some(CryptoStreamUtils.createKey(conf))
  } else {
    None
  }
  create(
    conf,
    SparkContext.DRIVER_IDENTIFIER,
    bindAddress,
    advertiseAddress,
    Option(port),
    isLocal,
    numCores,
    ioEncryptionKey,
    listenerBus = listenerBus,
    mockOutputCommitCoordinator = mockOutputCommitCoordinator
  )
}</pre>

### Executor环境创建
> leftSide=`true`


#### Executor环境的创建是通过org\.apache\.spark\.SparkEnv\.createExecutorEnv
- NOTE
<pre>private[spark] def createExecutorEnv(
    conf: SparkConf,
    executorId: String,
    hostname: String,
    numCores: Int,
    ioEncryptionKey: Option[Array[Byte]],
    isLocal: Boolean): SparkEnv = {
  val env = create(
    conf,
    executorId,
    hostname,
    hostname,
    None,
    isLocal,
    numCores,
    ioEncryptionKey
  )
  SparkEnv.set(env)
  env
}</pre>

### 初始化
> leftSide=`true`


#### SecurityManager

### 重要方法

#### create\(\)
- NOTE
<pre>private def create(
    conf: SparkConf,
    //executor的唯一标识，如果driver的话则是driver字符串
    executorId: String,
    //监听Socket的绑定端口
    bindAddress: String,
    //RPC端点地址
    advertiseAddress: String,
    //监听的端口号
    port: Option[Int],
    //是否是本地模式
    isLocal: Boolean, 
    //分配driver或executor的核心数
    numUsableCores: Int,
    //I/O加密的密钥，当spark.io.encryption.enable配置项启用时才有效
    ioEncryptionKey: Option[Array[Byte]],
    listenerBus: LiveListenerBus = null,
    mockOutputCommitCoordinator: Option[OutputCommitCoordinator] = None): SparkEnv</pre>

##### 按初始化顺序依次介绍

##### SecurityManager
- NOTE
<pre>val securityManager = new SecurityManager(conf, ioEncryptionKey)
if (isDriver) {
  securityManager.initializeAuth()
}

ioEncryptionKey.foreach { _ =&gt;
  if (!securityManager.isEncryptionEnabled()) {
    logWarning(&quot;I/O encryption enabled without RPC encryption: keys will be visible on the &quot; +
      &quot;wire.&quot;)
  }
}</pre>

###### 安全管理器

###### 通过共享密钥的方式进行认证
> leftSide=`true`


###### 基于ACL\(Access Control List,访问控制列表\),来管理Spark内部账号和权限

##### RpcEnv
> leftSide=`true`

- NOTE
<pre>val systemName = if (isDriver) driverSystemName else executorSystemName
val rpcEnv = RpcEnv.create(systemName, bindAddress, advertiseAddress, port.getOrElse(-1), conf, securityManager, numUsableCores, !isDriver)
// Figure out which port RpcEnv actually bound to in case the original port is 0 or occupied.
if (isDriver) {
  conf.set(&quot;spark.driver.port&quot;, rpcEnv.address.port.toString)
}
def create(
      name: String,
      bindAddress: String,
      advertiseAddress: String,
      port: Int,
      conf: SparkConf,
      securityManager: SecurityManager,
      numUsableCores: Int,
      clientMode: Boolean): RpcEnv = {
    val config = RpcEnvConfig(conf, name, bindAddress, advertiseAddress, port, securityManager,
      numUsableCores, clientMode)
    new NettyRpcEnvFactory().create(config)
}</pre>

###### RpcEnv即RPC环境，Spark各个组件之间必然会存在大量通信，这些通信实体被抽象成RPC端点和RPC引用

###### RpcEnv为RPC端点提供处理消息环境，并负责向RPC端点注册、端点销毁和端点间的消息路由

###### 底层是使用Netty来实现的

##### SerializerManager
> leftSide=`true`

- NOTE
<pre>val serializer = instantiateClassFromConf[Serializer](&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.JavaSerializer&quot;)
logDebug(s&quot;Using serializer: ${serializer.getClass}&quot;)
//数据序列化器
val serializerManager = new SerializerManager(serializer, conf, ioEncryptionKey)
//闭包序列化器
val closureSerializer = new JavaSerializer(conf)</pre>

###### 序列化器

###### Spark在存储或交换数据时，往往需要对数据进行序列化之后再反序列化
> leftSide=`true`


###### 为了节省空间还会对数据进行压缩
> leftSide=`true`


###### 序列化类型

####### org\.apache\.spark\.serializer\.JavaSerializer

####### org\.apache\.spark\.serializer\.KryoSerializer
> leftSide=`true`


###### 序列化器
> leftSide=`true`


####### serializerManager

######## 用于对数据序列化

####### closureSerializer
> leftSide=`true`


######## 闭包序列化器

######## 此序列化器通常用于DAGScheduler,TaskSetManager
> leftSide=`true`


######## 其序列化类型固定为org\.apache\.spark\.serializer\.JavaSerializer,不能修改
> leftSide=`true`


##### BroadcastManager

###### 广播管理器

###### 供用户广播数据，Spark Core内部也广泛使用
> leftSide=`true`


##### MapOutputTracker

###### Map端输出跟踪器

###### 在Shuffle过程中，Map端任务通过Shuffle Writer阶段产生中间数据
> leftSide=`true`


###### Reduce任务在Shuffle Read的时候需要知道哪些数据位于哪些节点上，以及Map输出的状态信息
> leftSide=`true`


###### MapOutputTracker负责维护以上这些信息
> leftSide=`true`


###### Driver端和Executor端

####### Driver端

######## MapOutputTrackerMaster

######## 接受Executor端发来的数据
> leftSide=`true`


####### Executor端
> leftSide=`true`


######## MapOutputTrackerWorker

######## 向Driver端汇报自己的情况
> leftSide=`true`


##### ShuffleManager
> leftSide=`true`


###### Shuffle管理器

###### 负责Shuffle阶段的机制，并提供具体实现
> leftSide=`true`


###### 默认使用排序sort,即SortShuffleManager

###### 主要是通过反射来构建shuffle实例
> leftSide=`true`


##### MemoryManager
> leftSide=`true`


###### 内存管理器

###### 负责Spark各个节点内存的分配、利用和回收
> leftSide=`true`


###### 内存管理机制
> leftSide=`true`


####### StaticMemoryManager，静态内存管理器\(1\.6\.x之前使用\)

####### UnifiedMemoryManager，统一内存管理器\(1\.6\.x之后使用，默认\)
> leftSide=`true`


##### BlockManager
> leftSide=`true`


###### 块管理器，负责块的存储、读写流程和状态信息

###### 块是Spark存储的基本单位
> leftSide=`true`


###### 在初始化BlockManager之前，还需要初始化块传输服务BlockTransportService，以及BlockManagerMaster
> leftSide=`true`


###### 采用主从结构，Driver上的是主RPC端点BlockManagerMasterEndpoint，Executor上的是从RPC端点BlockManagerSlaveEndpoint
> leftSide=`true`


###### Spark中的块与hdfs和操作系统中的块不太一样，Spark中的块可以存储在堆内存、堆外内存和外存\(磁盘\)
> leftSide=`true`


##### MetricsSystem
> leftSide=`true`

