Mind Map generated by NB MindMap plugin   
> __version__=`1.1`,showJumps=`true`
---

# Spark2\.4\.3

## SparkContext
- NOTE
<pre>SparkContext是Spark中极为重要的一个组件，它伴随着整个Spark程序的生命周期。
Spark的核心是RDD，而整个程序的第一个RDD就是由SparkContext来创建的。
SparkContext是通向Spark集群的入口。</pre>

### 主要属性
> collapsed=`true`


#### SparkConf
> collapsed=`true`


##### SparkConf是SparkContext初始化的必要前提

#### LiveListenerBus
> collapsed=`true`

- NOTE
<pre>_listenerBus = new LiveListenerBus(_conf)</pre>

##### 事件总线，异步的将产生的源事件投递给已经注册的监听器

#### AppStatusStore
> collapsed=`true`

- NOTE
<pre>// Initialize the app status store and listener before SparkEnv is created so that it gets
// all events.
_statusStore = AppStatusStore.createLiveStore(conf)
listenerBus.addToStatusQueue(_statusStore.listener.get)
//org.apache.spark.status.AppStatusStore.createLiveStore方法
def createLiveStore(conf: SparkConf): AppStatusStore = {
	//使用ElementTrackingStore数据结构，它是能够跟踪元素及其数量的键值对存储结构，因此适用于监控。
    val store = new ElementTrackingStore(new InMemoryStore(), conf)
    //产生一个AppStatusListener监听器，并注册到LiveListenerBus上用来收集监控数据
    val listener = new AppStatusListener(store, conf, true)
    new AppStatusStore(store, listener = Some(listener))
}</pre>

##### 它提供对Spark程序中各项指标的键值对监控，可以在SparkUI中看到其内容

#### SparkEnv
> collapsed=`true`

- NOTE
<pre>SparkEnv是Spark的执行环境，Driver和Executor的运行都需要其提供的个类组件形成的环境来作为基础
// Create the Spark execution environment (cache, map output tracker, etc)
//创建SparkEnv需要依赖LiveListenerBus
_env = createSparkEnv(_conf, isLocal, listenerBus)
//在创建Driver环境创建完毕之后，会调用SparkEnv伴生对象的set()方法保存它，这样就一处创建多处使用了。
SparkEnv.set(_env)
//org.apache.spark.SparkContext.createSparkEnv方法如下
// This function allows components created by SparkEnv to be mocked in unit tests:
private[spark] def createSparkEnv(
  conf: SparkConf,
  isLocal: Boolean,
  listenerBus: LiveListenerBus): SparkEnv = {
SparkEnv.createDriverEnv(conf, isLocal, listenerBus, SparkContext.numDriverCores(master, conf))
}
SparkEnv在初始化时只会先去初始化Driver的运行环境，SparkContext会借助SparkEnv来启动BlockManager和MetricManager</pre>

##### SecurityManager

##### RpcEnv
> collapsed=`true`


###### RpcEnv和ListenerBus一样，在Spark中不可获取

##### SerializerManager

##### BroadcastManager

##### MapOutputTracker

##### ShuffleManager

##### MemoryManager

##### BlockManager

##### MetricsSystem

##### OutputCommitCoordinator

#### SparkStatusTrack
> collapsed=`true`


##### Spark运行状态追踪器

##### 只提供很弱的一致性保证

#### ConsoleProgressBar
> collapsed=`true`


##### 在控制台打印Stage计算进度

##### 由spark\.ui\.showConsoleProgress参数控制，默认为false

#### SparkUI
> collapsed=`true`


##### 维护监控数据在WebUI上展示

##### 提供org\.apache\.spark\.ui\.WebUI\.WebUI参数来控制是否要开启WebUI ，默认为true

#### Configuration
> collapsed=`true`


##### 由SparkHadoopUtil初始化一些Hadoop相关配置存放到Hadoop的Configuration实例中

##### 参数以"spark\.hadoop"开头的配置

#### HeartbeatReceiver
> collapsed=`true`


##### 所有的Executor都会向HeartbeatReceiver发送心跳，以确保Executor还存活

##### 其本质也是个监听器，当收到Executor的心跳后，首先Executor的最后可见时间，将此信息交由TaskScheduler做进一步处理

##### 其被RpcEnv包装成了一个RpcEndpointRef，心跳机制时Spark集群中确认Executor状况的一种手段

#### SchedulerBackend
> collapsed=`true`

- TOPIC
<pre>16D56C43C01A</pre>

##### 负责向等待计算的Task分配计算资源，并在Executor上启动Task

##### SchedulerBackend是一个特质，根据不同的部署模式，会实例化相应的实现类
> collapsed=`true`


###### 本地模式\-\>LocalSchedulerBackend

###### 集群模式\-\>CoarseGrainedSchedulerBackend

###### StandaloneSchedulerBackend

#### TaskScheduler
> collapsed=`true`,topicLinkUID=`16D56C43C01A`

- NOTE
<pre>val (sched, ts) = SparkContext.createTaskScheduler(this, master, deployMode)
_schedulerBackend = sched
_taskScheduler = ts
/**
* 方法比较长，它包括有三种本地模式、本地集群模式、Standalone模式，以及第三方集群管理器（如YARN）提供的模式。
* Create a task scheduler based on a given master URL.
* Return a 2-tuple of the scheduler backend and the task scheduler.
*/
private def createTaskScheduler(
  sc: SparkContext,
  master: String,
  deployMode: String): (SchedulerBackend, TaskScheduler) = {
    import SparkMasterRegex._
    
    // When running locally, don't try to re-execute tasks on failure.
    val MAX_LOCAL_TASK_FAILURES = 1
    
    master match {
      case &quot;local&quot; =&gt;
        val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true)
        val backend = new LocalSchedulerBackend(sc.getConf, scheduler, 1)
        scheduler.initialize(backend)
        (backend, scheduler)
    
      case LOCAL_N_REGEX(threads) =&gt;
        def localCpuCount: Int = Runtime.getRuntime.availableProcessors()
        // local[*] estimates the number of cores on the machine; local[N] uses exactly N threads.
        val threadCount = if (threads == &quot;*&quot;) localCpuCount else threads.toInt
        if (threadCount &lt;= 0) {
          throw new SparkException(s&quot;Asked to run locally with $threadCount threads&quot;)
        }
        val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true)
        val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount)
        scheduler.initialize(backend)
        (backend, scheduler)
    
      case LOCAL_N_FAILURES_REGEX(threads, maxFailures) =&gt;
        def localCpuCount: Int = Runtime.getRuntime.availableProcessors()
        // local[*, M] means the number of cores on the computer with M failures
        // local[N, M] means exactly N threads with M failures
        val threadCount = if (threads == &quot;*&quot;) localCpuCount else threads.toInt
        val scheduler = new TaskSchedulerImpl(sc, maxFailures.toInt, isLocal = true)
        val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount)
        scheduler.initialize(backend)
        (backend, scheduler)
    
      case SPARK_REGEX(sparkUrl) =&gt;
        val scheduler = new TaskSchedulerImpl(sc)
        val masterUrls = sparkUrl.split(&quot;,&quot;).map(&quot;spark://&quot; + _)
        val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls)
        scheduler.initialize(backend)
        (backend, scheduler)
    
      case LOCAL_CLUSTER_REGEX(numSlaves, coresPerSlave, memoryPerSlave) =&gt;
        // Check to make sure memory requested &lt;= memoryPerSlave. Otherwise Spark will just hang.
        val memoryPerSlaveInt = memoryPerSlave.toInt
        if (sc.executorMemory &gt; memoryPerSlaveInt) {
          throw new SparkException(&quot;Asked to launch cluster with %d MB RAM / worker but requested %d MB/worker&quot;.format(memoryPerSlaveInt, sc.executorMemory))
        }
    
        val scheduler = new TaskSchedulerImpl(sc)
        val localCluster = new LocalSparkCluster(numSlaves.toInt, coresPerSlave.toInt, memoryPerSlaveInt, sc.conf)
        val masterUrls = localCluster.start()
        val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls)
        scheduler.initialize(backend)
        backend.shutdownCallback = (backend: StandaloneSchedulerBackend) =&gt; {
          localCluster.stop()
        }
        (backend, scheduler)
    
      case masterUrl =&gt;
        val cm = getClusterManager(masterUrl) match {
          case Some(clusterMgr) =&gt; clusterMgr
          case None =&gt; throw new SparkException(&quot;Could not parse Master URL: '&quot; + master + &quot;'&quot;)
        }
        try {
          val scheduler = cm.createTaskScheduler(sc, masterUrl)
          val backend = cm.createSchedulerBackend(sc, masterUrl, scheduler)
          cm.initialize(scheduler, backend)
          (backend, scheduler)
        } catch {
          case se: SparkException =&gt; throw se
          case NonFatal(e) =&gt;
            throw new SparkException(&quot;External scheduler cannot be instantiated&quot;, e)
        }
    }
}</pre>

##### TaskScheduler是任务调度器，它是一个特质，但只有一个实现TaskSchedulerImpl

##### 负责Task的调度算法，并且持有SchedulerBackend的实例，借由Backend来发挥作用

##### ClusterManager的资源分配属于一级分配，ClusterManager将Worker中的计算资源分配给Application，Application通过TaskScheduler进行二级分配，将计算资源分配给具体的Task

#### DAGScheduler
> collapsed=`true`

- NOTE
<pre>_dagScheduler = new DAGScheduler(this)
// start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler's constructor
_taskScheduler.start()</pre>
- TOPIC
<pre>16D56C43C01A</pre>

##### 有向无环图调度器，用来表示RDD之间的血缘关系

##### 负责生成并提交Job，以及按照DAG将RDD和算子划分成Stage\(依照宽依赖来划分Stage\)

##### 每个Stage包含一组Task，称之为TaskSet，它们被传递到TaskScheduler

##### TaskScheduler所调度的Task是由DAGScheduler创建的，DAGScheduler是TaskScheduler的前置调度

##### DAGScheduler的初始化是直接被new出来的,但在其构造方法里也会将SparkContext中TaskScheduler的引用传进去。因此要等DAGScheduler创建后，再真正启动TaskScheduler。

#### EventLoggingListener
> collapsed=`true`


##### 用于事件持久化的监听器

##### 可以通过spark\.eventLog\.enabled参数来控制是否要开启，默认false，如果开启，它会注册到LiveListenerBus中，并将一些特定事件持久化到磁盘中

#### ExecutorAllocationManager
> collapsed=`true`

- NOTE
<pre>// Optionally scale number of executors dynamically based on workload. Exposed for testing.
val dynamicAllocationEnabled = Utils.isDynamicAllocationEnabled(_conf)
_executorAllocationManager =
  if (dynamicAllocationEnabled) {
    schedulerBackend match {
      case b: ExecutorAllocationClient =&gt;
        Some(new ExecutorAllocationManager(schedulerBackend.asInstanceOf[ExecutorAllocationClient], listenerBus, _conf,env.blockManager.master))
      case _ =&gt;
        None
    }
  } else {
    None
  }
_executorAllocationManager.foreach(_.start())</pre>

##### 是Executor的动态分配管理器，Spark会根据程序的负载情况来动态的增加或减少Executor的数量

#### ContextCleaner
> collapsed=`true`


##### 上下文清理器，内部维护着对RDD、Shuffle依赖和广播变量的弱引用，如果弱引用超出了程序的作用域，就异步的将其清理掉

##### 可以通过spark\.cleaner\.referenceTracking配置来控制是否要开启，默认是true

### 辅助属性
> collapsed=`true`


#### creationSite
> collapsed=`true`


##### org\.apache\.spark\.util\.CallSite

##### 用来描述SparkContext在哪被则创建了，描述的是代码的位置

#### allowMultipleContexts
> collapsed=`true`


##### 其表示一个JVM之中是否允许允许多个SparkContext实例

##### 可以通过spark\.driver\.allowMultipleContexts来进行配置，默认是false，如果设置成true的话会有警告

#### addJars/addFiles&\_jars/\_files
> collapsed=`true`

- NOTE
<pre>// Used to store a URL for each static file/jar together with the file's local timestamp
private[spark] val addedFiles = new ConcurrentHashMap[String, Long]().asScala
private[spark] val addedJars = new ConcurrentHashMap[String, Long]().asScala

//首先获取SparkConf中的spark.jars和spark.files的内容
_jars = Utils.getUserJars(_conf)
/**
* Return the jar files pointed by the &quot;spark.jars&quot; property. Spark internally will distribute
* these jars through file server. In the YARN mode, it will return an empty list, since YARN
* has its own mechanism to distribute jars.
*/
def getUserJars(conf: SparkConf): Seq[String] = {
    val sparkJars = conf.getOption(&quot;spark.jars&quot;)
    sparkJars.map(_.split(&quot;,&quot;)).map(_.filter(_.nonEmpty)).toSeq.flatten
}
_files = _conf.getOption(&quot;spark.files&quot;).map(_.split(&quot;,&quot;)).map(_.filter(_.nonEmpty)).toSeq.flatten</pre>

##### Spark支持在提交应用的时候用户提交自定义的文件和jar包

##### addJars/addFiles其数据结构是ConcurrentHashMap，用来维护用户自定义的jar和文件的URL路径和时间戳

##### \_jars/\_files分别接收spark\.jars/spark\.files开头的参数

#### persistentRdds
> collapsed=`true`

- NOTE
<pre>persistentRdds的初始化代码
private[spark] val persistentRdds = {
    val map: ConcurrentMap[Int, RDD[_]] = new MapMaker().weakValues().makeMap[Int, RDD[_]]()
    map.asScala
}
//持久化和反持久化一个RDD的方法
private[spark] def persistRDD(rdd: RDD[_]) {
//最终是向persistentRdds中的ConcurrentHashMap增加rdd的id
persistentRdds(rdd.id) = rdd
}

/**
* Unpersist an RDD from memory and/or disk storage
* 最终是把persistentRdds中对应的rdd的id移除，并向消息总线投递一个反持久化一个RDD的消息
*/
private[spark] def unpersistRDD(rddId: Int, blocking: Boolean = true) {
env.blockManager.master.removeRdd(rddId, blocking)
persistentRdds.remove(rddId)
listenerBus.post(SparkListenerUnpersistRDD(rddId))
}</pre>

##### Spark支持RDD的持久化，可以将RDD持久化到磁盘或内存中

##### persistentRdds内部维护着一个ConcurrentHashMap用来维护持久化的RDD的ID与其弱引用的映射关系

##### RDD可以通过自带的cache\(\)/persisit\(\)/unpersisit\(\)方法持久化与反持久化一个RDD，其底层就是操作persistentRdds中的ConcurrentHashMap

#### executeEnvs&\_executorMemory&\_sparkUser
> collapsed=`true`

- NOTE
<pre>private[spark] val executorEnvs = HashMap[String, String]()
//Executor内存可以通过spark.executor.memory配置项、SPARK_EXECUTOR_MEMORY环境变量、SPARK_MEM环境变量指定，优先级依次降低，且默认大小是1GB1024MB。
_executorMemory = _conf.getOption(&quot;spark.executor.memory&quot;)
                       .orElse(Option(System.getenv(&quot;SPARK_EXECUTOR_MEMORY&quot;)))
                       .orElse(Option(System.getenv(&quot;SPARK_MEM&quot;))
                       .map(warnSparkMem))
                       .map(Utils.memoryStringToMb)
                       .getOrElse(1024)
val sparkUser = Utils.getCurrentUserName()
//executorEnvs也会存储_executorMemory和_sparkUser
executorEnvs(&quot;SPARK_EXECUTOR_MEMORY&quot;) = executorMemory + &quot;m&quot;
executorEnvs ++= _conf.getExecutorEnv
executorEnvs(&quot;SPARK_USER&quot;) = sparkUser</pre>

##### executeEnv是一个HashMap，用来存储需要传递Executor的环境变量，将传递给正在执行Task的Executor使用

##### \_executorMemory&\_sparkUser就是executeEnv中的内容，分别代表Executor的内存大小和启动Spark的用户名

##### \_executorMemory默认大小是1G

#### checkpointDir
> collapsed=`true`

- NOTE
<pre>def setCheckpointDir(directory: String) {

    // If we are running on a cluster, log a warning if the directory is local.
    // Otherwise, the driver may attempt to reconstruct the checkpointed RDD from
    // its own local file system, which is incorrect because the checkpoint files
    // are actually on the executor machines.
    if (!isLocal &amp;&amp; Utils.nonLocalPaths(directory).isEmpty) {
      logWarning(&quot;Spark is not running in local mode, therefore the checkpoint directory &quot; +
        s&quot;must not be on the local filesystem. Directory '$directory' &quot; +
        &quot;appears to be on the local filesystem.&quot;)
    }

    checkpointDir = Option(directory).map { dir =&gt;
      val path = new Path(dir, UUID.randomUUID().toString)
      val fs = path.getFileSystem(hadoopConfiguration)
      fs.mkdirs(path)
      fs.getFileStatus(path).getPath.toString
    }
  }</pre>

##### 指定Spark的检查点的文件夹所在的hdfs路径

##### 为了能够在计算过程中如果出错的话，能够从检查点快速恢复，而不必从头重新计算整个RDD的lineage

##### SparkContext提供setCheckpointDir\(\)方法来设置检查点路径

##### 一旦设立检查点之后，Spark就会切断checkpoint之前的lineage，下次直接从checkpoint开始恢复

#### localProperties
> collapsed=`true`


##### 维护一个properties数据类型的线程本地变量，它是InheritableThreadLocal类型，继承自ThreadLocal，在后者的基础上允许本地变量从父线程到子线程的继承，也就是该Properties会沿着线程栈传递下去。

#### \_applicationId&\_applicationAttemptId
> collapsed=`true`

- NOTE
<pre>_applicationId = _taskScheduler.applicationId()
_applicationAttemptId = taskScheduler.applicationAttemptId()</pre>

##### 这两个ID都是在TaskScheduler初始化并启动之后再分配，TaskScheduler启动之后，应用的代码逻辑才真正被执行，并且可以多次尝试

#### \_shutdownHookRef
> collapsed=`true`


##### Spark程序的关闭钩子，在主动退出JVM的时候进行一些资源的清理

#### nextShuffleId&nextRddId
> collapsed=`true`

- NOTE
<pre>private val nextShuffleId = new AtomicInteger(0)
private val nextRddId = new AtomicInteger(0)</pre>

##### 这两个ID都是AtomicInteger类型保证原子性。Shuffle和RDD都需要唯一ID来进行标识，并且它们是递增的

### 后置初始化
> collapsed=`true`

- NOTE
<pre>    setupAndStartListenerBus()
    postEnvironmentUpdate()
    postApplicationStart()

    // Post init
    //等待SchedulerBackend初始化完毕
    _taskScheduler.postStartHook()
    //在度量系统注册DAGScheduler、BlockManager、ExecutorAllocationManager的度量源，以收集它们的监控指标
    _env.metricsSystem.registerSource(_dagScheduler.metricsSource)
    _env.metricsSystem.registerSource(new BlockManagerSource(_env.blockManager))
    _executorAllocationManager.foreach { e =&gt;
      _env.metricsSystem.registerSource(e.executorAllocationManagerSource)
    }

    // Make sure the context is stopped if the user forgets about it. This avoids leaving
    // unfinished event logs around after the JVM exits cleanly. It doesn't help if the JVM
    // is killed, though.
    logDebug(&quot;Adding shutdown hook&quot;) // force eager creation of logger
    //添加关闭钩子
    _shutdownHookRef = ShutdownHookManager.addShutdownHook(
      ShutdownHookManager.SPARK_CONTEXT_SHUTDOWN_PRIORITY) { () =&gt;
      logInfo(&quot;Invoking stop() from shutdown hook&quot;)
      try {
        stop()
      } catch {
        case e: Throwable =&gt;
          logWarning(&quot;Ignoring Exception while stopping SparkContext from shutdown hook&quot;, e)
      }
    }
  //标记SparkContext为活动状态
  SparkContext.setActiveContext(this, allowMultipleContexts)</pre>

#### org\.apache\.spark\.SparkContext\#setupAndStartListenerBus
> collapsed=`true`

- NOTE
<pre>  private def setupAndStartListenerBus(): Unit = {
    try {
      //获取用户自定义监听器并遍历
      conf.get(EXTRA_LISTENERS).foreach { classNames =&gt;
        //通过反射监听器
        val listeners = Utils.loadExtensions(classOf[SparkListenerInterface], classNames, conf)
        listeners.foreach { listener =&gt;
          //注册到LiveListenerBus中
          listenerBus.addToSharedQueue(listener)
          logInfo(s&quot;Registered listener ${listener.getClass().getName()}&quot;)
        }
      }
    } catch {
      case e: Exception =&gt;
        try {
          stop()
        } finally {
          throw new SparkException(s&quot;Exception when registering SparkListener&quot;, e)
        }
    }

    listenerBus.start(this, _env.metricsSystem)
    _listenerBusStarted = true
  }</pre>

##### 这个方法用于注册用户注册自定义监听器，并最终启动LiveListenerBus

##### 自定义监听器都需要实现SparkListener特质，通过spark\.extralListeners来配置

##### 然后通过Utils\.loadExtensions\(\)方法来通过反射来构建监听器的实例，并将它注册到LiveListenerBus中

#### org\.apache\.spark\.SparkContext\#postEnvironmentUpdate
> collapsed=`true`

- NOTE
<pre>  private def postEnvironmentUpdate() {
    if (taskScheduler != null) {
      //获取调度方式
      val schedulingMode = getSchedulingMode.toString
      //获得jar列表
      val addedJarPaths = addedJars.keys.toSeq
      //获得文件列表
      val addedFilePaths = addedFiles.keys.toSeq
      //一些环境细节
      val environmentDetails = SparkEnv.environmentDetails(conf, schedulingMode, addedJarPaths, addedFilePaths)
      //封装成SparkListenerEnvironmentUpdate事件
      val environmentUpdate = SparkListenerEnvironmentUpdate(environmentDetails)
      //推送到事件总线LiveListenerBus中
      listenerBus.post(environmentUpdate)
    }
  }</pre>

##### 该方法在添加自定义文件和jar的时候也会调用，因为添加新的资源会对程序的执行环境产生影响

##### 它会取得当前的自定义文件和JAR包列表，以及Spark配置、调度方式，然后通过SparkEnv\.environmentDetails\(\)方法再取得JVM参数、Java系统属性等，一同封装成SparkListenerEnvironmentUpdate事件，并投递给事件总线。

#### org\.apache\.spark\.SparkContext\#postApplicationStart
> collapsed=`true`

- NOTE
<pre>  /** Post the application start event */
  private def postApplicationStart() {
    // Note: this code assumes that the task scheduler has been initialized and has contacted
    // the cluster manager to get an application ID (in case the cluster manager provides one).
    listenerBus.post(SparkListenerApplicationStart(appName, Some(applicationId),
      startTime, sparkUser, applicationAttemptId, schedulerBackend.getDriverLogUrls))
  }</pre>

##### 向事件总线投递SparkListenerApplicationStart事件，表示Application已经启动。

### 其他功能
> collapsed=`true`


#### 生成RDD
> collapsed=`true`


##### 对内存中的数据执行并行化\(parallelize\)操作
- NOTE
<pre>  def parallelize[T: ClassTag](
      seq: Seq[T],
      numSlices: Int = defaultParallelism): RDD[T] = withScope {
    assertNotStopped()
    new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())
  }</pre>

##### 从外部存储系统中读取数据之后生成RDD
- NOTE
<pre>  def textFile(
      path: String,
      minPartitions: Int = defaultMinPartitions): RDD[String] = withScope {
    assertNotStopped()
    //使用TextInputFormat来接收读到的文件中的数据，使用LongWritable来接收文件内容的偏移量
    hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text], minPartitions).map(pair =&gt; pair._2.toString).setName(path)
  }
  def hadoopFile[K, V](
      path: String,
      inputFormatClass: Class[_ &lt;: InputFormat[K, V]],
      keyClass: Class[K],
      valueClass: Class[V],
      minPartitions: Int = defaultMinPartitions): RDD[(K, V)] = withScope {
    assertNotStopped()
    // This is a hack to enforce loading hdfs-site.xml.
    // See SPARK-11227 for details.
    FileSystem.getLocal(hadoopConfiguration)
    // A Hadoop configuration can be about 10 KB, which is pretty big, so broadcast it.
    val confBroadcast = broadcast(new SerializableConfiguration(hadoopConfiguration))
    val setInputPathsFunc = (jobConf: JobConf) =&gt; FileInputFormat.setInputPaths(jobConf, path)
    new HadoopRDD(
      this,
      confBroadcast,
      Some(setInputPathsFunc),
      inputFormatClass,
      keyClass,
      valueClass,
      minPartitions).setName(path)
  }</pre>

##### 在生成RDD的时候可以指定RDD中有多少个分区

#### 广播变量
> collapsed=`true`

- NOTE
<pre>  def broadcast[T: ClassTag](value: T): Broadcast[T] = {
    assertNotStopped()
    require(!classOf[RDD[_]].isAssignableFrom(classTag[T].runtimeClass),
      &quot;Can not directly broadcast RDDs; instead, call collect() and broadcast the result.&quot;)
    val bc = env.broadcastManager.newBroadcast[T](value, isLocal)
    val callSite = getCallSite
    logInfo(&quot;Created broadcast &quot; + bc.id + &quot; from &quot; + callSite.shortForm)
    cleaner.foreach(_.registerBroadcastForCleanup(bc))
    bc
  }</pre>

##### 广播变量是Spark中两种共享变量的一种

##### 所谓广播就是Driver向每个Executor发送一份数据的只读副本，而不需要Task来计算获取

##### 广播变量适合处理多节点跨Stage的共享数据，特别是输入量较大的数据，可以提高效率

#### 累加器
> collapsed=`true`


##### 累加器是Spark中两种共享变量的一种

##### 累加器就是一个能够累加结果的变量，最常见的用途是计数

##### 累加器在Driver创建和读取，在Executor端更新即做累加操作

##### 所有累计器都继承自AccumulatorV2抽象类，我们也可以自定义累加器

#### runJob
> collapsed=`true`

- NOTE
<pre>  def runJob[T, U: ClassTag](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) =&gt; U,
      partitions: Seq[Int],
      resultHandler: (Int, U) =&gt; Unit): Unit = {
    if (stopped.get()) {
      throw new IllegalStateException(&quot;SparkContext has been shutdown&quot;)
    }
    val callSite = getCallSite
    val cleanedFunc = clean(func)
    logInfo(&quot;Starting job: &quot; + callSite.shortForm)
    if (conf.getBoolean(&quot;spark.logLineage&quot;, false)) {
      logInfo(&quot;RDD's recursive dependencies:\n&quot; + rdd.toDebugString)
    }
    dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)
    progressBar.foreach(_.finishAll())
    rdd.doCheckpoint()
  }</pre>

##### Spark提供很多种runJob的重载来允许一个Job，即触发Action算子的执行

##### runJob其实调用的DAGScheduler中的runJob方法来允许Job，它接收需要计算的RDD的分区列表

##### 在计算完成后，将结果传回给resultHandler回调方法。在运行Job的同时，还会对RDD本身保存检查点

### SparkContext的伴生对象
- NOTE
<pre>伴生对象主要用来跟踪并维护SparkContext的创建与激活</pre>

#### org\.apache\.spark\.SparkContext\#createTaskScheduler

##### 创建TaskScheduler

## RDD
> collapsed=`true`,leftSide=`true`


### A list of partitions

### A function for computing each split
> leftSide=`true`


### A list of dependencies on other RDDs
> leftSide=`true`


### Optionally, a Partitioner for key\-value RDDs
> leftSide=`true`


### Optionally, a list of preferred locations to compute each split on
> leftSide=`true`


## SparkConf
> collapsed=`true`,leftSide=`true`


### 可以直接从Java的系统属性中获取数据

### SparkConf中的key和value都不能为null

### SparkConf提供set和get方法来设置和获取属性值
> leftSide=`true`


### SparkConf会检查配置项时候过期了或配置项不符合要求，从而发出警告
> leftSide=`true`

